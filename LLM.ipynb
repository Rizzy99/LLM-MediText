{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K29HKgqDrifZ",
        "outputId": "cabef535-373a-4fb2-e88d-5014e80efca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
            "  Downloading cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pymupdf, cryptography, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 3.4.8\n",
            "    Uninstalling cryptography-3.4.8:\n",
            "      Successfully uninstalled cryptography-3.4.8\n",
            "Successfully installed cryptography-44.0.0 pdfminer.six-20231228 pdfplumber-0.11.5 pymupdf-1.25.2 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "pip install pdfminer.six pymupdf pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dne8JqXjru3l",
        "outputId": "d458d0d4-c817-4653-9e05-2e6fb2259393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted data saved to extracted_medical_text3.json\n"
          ]
        }
      ],
      "source": [
        "import pdfminer.high_level\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Function to extract structured text while streaming to save memory\n",
        "def extract_text_with_structure(pdf_path, start_page=0, end_page=None):\n",
        "    \"\"\"Stream text extraction to save memory.\"\"\"\n",
        "    structured_text = []\n",
        "\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        text = pdfminer.high_level.extract_text(f, page_numbers=range(start_page, end_page))\n",
        "        for line in text.split(\"\\n\"):\n",
        "            if re.match(r\"^\\d+\\.\\s+\\w+\", line):  # Heading detection (e.g., \"1. Introduction\")\n",
        "                structured_text.append({\"type\": \"heading\", \"text\": line.strip()})\n",
        "            elif line.strip():\n",
        "                structured_text.append({\"type\": \"paragraph\", \"text\": line.strip()})\n",
        "\n",
        "    return structured_text\n",
        "\n",
        "# Function to extract tables in smaller batches\n",
        "def extract_tables(pdf_path, start_page=0, end_page=None):\n",
        "    \"\"\"Extract tables in batches to reduce memory usage.\"\"\"\n",
        "    tables_data = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        if end_page is None:\n",
        "            end_page = len(pdf.pages)\n",
        "\n",
        "        for i in range(start_page, min(end_page, len(pdf.pages))):\n",
        "            tables = pdf.pages[i].extract_tables()\n",
        "            for table in tables:\n",
        "                tables_data.append({\"page\": i + 1, \"table\": table})\n",
        "\n",
        "    return tables_data\n",
        "\n",
        "# Function to extract and save images instead of storing them in memory\n",
        "def extract_figures(pdf_path, start_page=0, end_page=None, output_folder=\"figures3\"):\n",
        "    \"\"\"Extract images and save to disk to reduce memory usage.\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    figures = []\n",
        "\n",
        "    for i in range(start_page, min(end_page, len(doc))):\n",
        "        for img_index, img in enumerate(doc[i].get_images(full=True)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            img_filename = os.path.join(output_folder, f\"page_{i+1}_img_{img_index}.png\")\n",
        "\n",
        "            with open(img_filename, \"wb\") as img_file:\n",
        "                img_file.write(base_image[\"image\"])\n",
        "\n",
        "            figures.append({\"page\": i + 1, \"image_index\": img_index, \"file_path\": img_filename})\n",
        "\n",
        "    return figures\n",
        "\n",
        "# Function to process a specific range of pages\n",
        "def process_page_range(pdf_path, start, end):\n",
        "    return {\n",
        "        \"text\": extract_text_with_structure(pdf_path, start, end),\n",
        "        \"tables\": extract_tables(pdf_path, start, end),\n",
        "        \"figures\": extract_figures(pdf_path, start, end)\n",
        "    }\n",
        "\n",
        "# Function to extract and save all data efficiently\n",
        "def save_extracted_data(pdf_path, output_json=\"extracted_medical_text3.json\", batch_size=500):\n",
        "    \"\"\"Parallelize extraction to improve speed and efficiency.\"\"\"\n",
        "    num_pages = fitz.open(pdf_path).page_count\n",
        "    ranges = [(pdf_path, i, min(i + batch_size, num_pages)) for i in range(0, num_pages, batch_size)]\n",
        "\n",
        "    with Pool(processes=4) as pool:  # Adjust the number of processes based on CPU\n",
        "        results = pool.starmap(process_page_range, ranges)\n",
        "\n",
        "    # Merge results\n",
        "    extracted_data = {\n",
        "        \"text\": sum([res[\"text\"] for res in results], []),\n",
        "        \"tables\": sum([res[\"tables\"] for res in results], []),\n",
        "        \"figures\": sum([res[\"figures\"] for res in results], [])\n",
        "    }\n",
        "\n",
        "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(extracted_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Extracted data saved to {output_json}\")\n",
        "\n",
        "# Run the extraction on your PDF\n",
        "pdf_file = \"/content/Book3.pdf\"  # Replace with your actual PDF path\n",
        "save_extracted_data(pdf_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrQWoZ0UGzj4",
        "outputId": "ffaf369b-d0d3-4b38-f4be-1592fdc343e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.11)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pymongo) (2.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pymongo import MongoClient\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load extracted medical text\n",
        "def load_corpus(filename=\"extracted_medical_text1.json\"):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return [item[\"text\"] for item in data[\"text\"]]\n",
        "\n",
        "# Preprocess text for BM25 indexing\n",
        "def preprocess_text(texts):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def clean_text(text):\n",
        "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "        return [lemmatizer.lemmatize(word) for word in words]\n",
        "    return [clean_text(text) for text in texts]\n",
        "\n",
        "# Query Expansion using WordNet synonyms\n",
        "def expand_query(query):\n",
        "    synonyms = set()\n",
        "    for word in query.split():\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name().replace('_', ' '))\n",
        "    return query + \" \" + \" \".join(synonyms)\n",
        "\n",
        "# Initialize BM25\n",
        "def init_bm25(corpus):\n",
        "    tokenized_corpus = preprocess_text(corpus)\n",
        "    return BM25Okapi(tokenized_corpus), tokenized_corpus\n",
        "\n",
        "# Initialize Sentence-BERT for dense retrieval\n",
        "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def encode_corpus_sbert(corpus):\n",
        "    return sbert_model.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Retrieve documents using BM25 and Sentence-BERT\n",
        "def hybrid_retrieval(query, bm25, tokenized_corpus, corpus, sbert_embeddings, top_k=10):\n",
        "    query_expanded = expand_query(query)\n",
        "    bm25_scores = bm25.get_scores(query_expanded.split())\n",
        "    sbert_query_embedding = sbert_model.encode(query, convert_to_tensor=True)\n",
        "    sbert_scores = util.pytorch_cos_sim(sbert_query_embedding, sbert_embeddings)[0].numpy()\n",
        "    \n",
        "    combined_scores = bm25_scores + sbert_scores  # Hybrid approach\n",
        "    top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
        "    \n",
        "    return [(corpus[i], combined_scores[i]) for i in top_indices]\n",
        "\n",
        "# Save results to MongoDB\n",
        "def save_results_to_mongo(results, query, mongo_db=\"retrieval_db\", collection_name=\"results\"):\n",
        "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "    db = client[mongo_db]\n",
        "    collection = db[collection_name]\n",
        "    collection.insert_one({\"query\": query, \"results\": results})\n",
        "    print(\"Results saved to MongoDB.\")\n",
        "\n",
        "# Main pipeline\n",
        "def main():\n",
        "    corpus = load_corpus()\n",
        "    bm25, tokenized_corpus = init_bm25(corpus)\n",
        "    sbert_embeddings = encode_corpus_sbert(corpus)\n",
        "    \n",
        "    query = \"heart attack symptoms\"\n",
        "    results = hybrid_retrieval(query, bm25, tokenized_corpus, corpus, sbert_embeddings)\n",
        "    \n",
        "    for doc, score in results:\n",
        "        print(f\"Score: {score:.4f} - {doc[:200]}...\")  # Show preview\n",
        "    \n",
        "    save_results_to_mongo(results, query)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ss-XCuTTy32G",
        "outputId": "a2affff5-0dbe-41a2-f123-93ae721ebe47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hierarchical tree saved to hierarchical_tree.json\n",
            "Hierarchical tree stored in hierarchical_tree.db\n",
            "Hierarchical tree saved to MongoDB (DB: textbook_db, Collection: hierarchical_tree)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "from pymongo import MongoClient\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Represents a node in the hierarchical tree.\"\"\"\n",
        "    def __init__(self, node_id, text, level, parent=None):\n",
        "        self.node_id = node_id\n",
        "        self.text = text\n",
        "        self.level = level  # 0: root, 1: chapter, 2: section, 3: subsection, 4: paragraph\n",
        "        self.children = []\n",
        "        self.parent = parent\n",
        "\n",
        "    def add_child(self, child):\n",
        "        \"\"\"Adds a child node.\"\"\"\n",
        "        self.children.append(child)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Converts node to dictionary format for JSON storage.\"\"\"\n",
        "        return {\n",
        "            \"node_id\": self.node_id,\n",
        "            \"text\": self.text,\n",
        "            \"level\": self.level,\n",
        "            \"children\": [child.to_dict() for child in self.children]\n",
        "        }\n",
        "\n",
        "class HierarchicalTree:\n",
        "    \"\"\"Creates and manages the hierarchical structure of the textbook.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.root = Node(\"root\", \"Textbook\", 0)\n",
        "\n",
        "    def parse_structure(self, extracted_data):\n",
        "        \"\"\"Parses structured text and organizes it into a hierarchical tree.\"\"\"\n",
        "        current_chapter = None\n",
        "        current_section = None\n",
        "        current_subsection = None\n",
        "        node_counter = 1  # Unique identifier for nodes\n",
        "\n",
        "        for item in extracted_data[\"text\"]:\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            # Identify structure based on patterns\n",
        "            if re.match(r\"^\\d+\\.\\s+[A-Z]\", text):  # Chapter (e.g., \"1. Introduction\")\n",
        "                current_chapter = Node(f\"ch_{node_counter}\", text, 1, self.root)\n",
        "                self.root.add_child(current_chapter)\n",
        "                current_section = None\n",
        "                current_subsection = None\n",
        "                node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\s+[A-Z]\", text):  # Section (e.g., \"1.1 Background\")\n",
        "                if current_chapter:\n",
        "                    current_section = Node(f\"sec_{node_counter}\", text, 2, current_chapter)\n",
        "                    current_chapter.add_child(current_section)\n",
        "                    current_subsection = None\n",
        "                    node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\.\\d+\\s+[A-Z]\", text):  # Subsection (e.g., \"1.1.1 Definition\")\n",
        "                if current_section:\n",
        "                    current_subsection = Node(f\"subsec_{node_counter}\", text, 3, current_section)\n",
        "                    current_section.add_child(current_subsection)\n",
        "                    node_counter += 1\n",
        "\n",
        "            else:  # Paragraphs\n",
        "                if current_subsection:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_subsection)\n",
        "                    current_subsection.add_child(paragraph_node)\n",
        "                elif current_section:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_section)\n",
        "                    current_section.add_child(paragraph_node)\n",
        "                elif current_chapter:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_chapter)\n",
        "                    current_chapter.add_child(paragraph_node)\n",
        "                else:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, self.root)\n",
        "                    self.root.add_child(paragraph_node)\n",
        "\n",
        "                node_counter += 1\n",
        "\n",
        "    def to_json(self, filename=\"hierarchical_tree.json\"):\n",
        "        \"\"\"Stores hierarchical structure as JSON.\"\"\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.root.to_dict(), f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Hierarchical tree saved to {filename}\")\n",
        "\n",
        "    def save_to_sqlite(self, db_name=\"hierarchical_tree.db\"):\n",
        "        \"\"\"Stores hierarchical structure in SQLite.\"\"\"\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS hierarchy (\n",
        "            node_id TEXT PRIMARY KEY,\n",
        "            text TEXT,\n",
        "            level INTEGER,\n",
        "            parent_id TEXT\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        def insert_node(node, parent_id=None):\n",
        "            cursor.execute(\"INSERT INTO hierarchy VALUES (?, ?, ?, ?)\", (node.node_id, node.text, node.level, parent_id))\n",
        "            for child in node.children:\n",
        "                insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Hierarchical tree stored in {db_name}\")\n",
        "\n",
        "    def save_to_mongodb(self, mongo_db=\"textbook_db\", collection_name=\"hierarchical_tree\"):\n",
        "        \"\"\"Stores hierarchical structure in MongoDB.\"\"\"\n",
        "        client = MongoClient(\"mongodb+srv://anant22067:db6aM9UfKxitrBw0@cluster0.oyo8k.mongodb.net/\")\n",
        "        db = client[mongo_db]\n",
        "        collection = db[collection_name]\n",
        "\n",
        "        collection.delete_many({})  # Clear previous data\n",
        "        def insert_node(node, parent_id=None):\n",
        "                \"\"\"Recursively insert nodes into MongoDB as separate documents.\"\"\"\n",
        "                document = {\n",
        "                    \"node_id\": node.node_id,\n",
        "                    \"text\": node.text,\n",
        "                    \"level\": node.level,\n",
        "                    \"parent_id\": parent_id  # Store parent reference\n",
        "                }\n",
        "                collection.insert_one(document)\n",
        "\n",
        "                for child in node.children:\n",
        "                    insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)  # Start inserting from the root node\n",
        "\n",
        "        print(f\"Hierarchical tree saved to MongoDB (DB: {mongo_db}, Collection: {collection_name})\")\n",
        "\n",
        "# Usage Example\n",
        "if __name__ == \"__main__\":\n",
        "    # Load extracted data from JSON\n",
        "    with open(\"extracted_medical_text1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        extracted_data = json.load(f)\n",
        "\n",
        "    # Build hierarchical tree\n",
        "    tree = HierarchicalTree()\n",
        "    tree.parse_structure(extracted_data)\n",
        "\n",
        "    # Save to different storage formats\n",
        "    tree.to_json()  # Save as JSON\n",
        "    tree.save_to_sqlite()  # Save to SQLite\n",
        "    tree.save_to_mongodb()  # Save to MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hierarchical tree saved to hierarchical_tree2.json\n",
            "Hierarchical tree stored in hierarchical_tree2.db\n",
            "Hierarchical tree saved to MongoDB (DB: textbook2_db, Collection: hierarchical_tree2)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "from pymongo import MongoClient\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Represents a node in the hierarchical tree.\"\"\"\n",
        "    def __init__(self, node_id, text, level, parent=None):\n",
        "        self.node_id = node_id\n",
        "        self.text = text\n",
        "        self.level = level  # 0: root, 1: chapter, 2: section, 3: subsection, 4: paragraph\n",
        "        self.children = []\n",
        "        self.parent = parent\n",
        "\n",
        "    def add_child(self, child):\n",
        "        \"\"\"Adds a child node.\"\"\"\n",
        "        self.children.append(child)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Converts node to dictionary format for JSON storage.\"\"\"\n",
        "        return {\n",
        "            \"node_id\": self.node_id,\n",
        "            \"text\": self.text,\n",
        "            \"level\": self.level,\n",
        "            \"children\": [child.to_dict() for child in self.children]\n",
        "        }\n",
        "\n",
        "class HierarchicalTree:\n",
        "    \"\"\"Creates and manages the hierarchical structure of the textbook.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.root = Node(\"root\", \"Textbook\", 0)\n",
        "\n",
        "    def parse_structure(self, extracted_data):\n",
        "        \"\"\"Parses structured text and organizes it into a hierarchical tree.\"\"\"\n",
        "        current_chapter = None\n",
        "        current_section = None\n",
        "        current_subsection = None\n",
        "        node_counter = 1  # Unique identifier for nodes\n",
        "\n",
        "        for item in extracted_data[\"text\"]:\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            # Identify structure based on patterns\n",
        "            if re.match(r\"^\\d+\\.\\s+[A-Z]\", text):  # Chapter (e.g., \"1. Introduction\")\n",
        "                current_chapter = Node(f\"ch_{node_counter}\", text, 1, self.root)\n",
        "                self.root.add_child(current_chapter)\n",
        "                current_section = None\n",
        "                current_subsection = None\n",
        "                node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\s+[A-Z]\", text):  # Section (e.g., \"1.1 Background\")\n",
        "                if current_chapter:\n",
        "                    current_section = Node(f\"sec_{node_counter}\", text, 2, current_chapter)\n",
        "                    current_chapter.add_child(current_section)\n",
        "                    current_subsection = None\n",
        "                    node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\.\\d+\\s+[A-Z]\", text):  # Subsection (e.g., \"1.1.1 Definition\")\n",
        "                if current_section:\n",
        "                    current_subsection = Node(f\"subsec_{node_counter}\", text, 3, current_section)\n",
        "                    current_section.add_child(current_subsection)\n",
        "                    node_counter += 1\n",
        "\n",
        "            else:  # Paragraphs\n",
        "                if current_subsection:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_subsection)\n",
        "                    current_subsection.add_child(paragraph_node)\n",
        "                elif current_section:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_section)\n",
        "                    current_section.add_child(paragraph_node)\n",
        "                elif current_chapter:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_chapter)\n",
        "                    current_chapter.add_child(paragraph_node)\n",
        "                else:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, self.root)\n",
        "                    self.root.add_child(paragraph_node)\n",
        "\n",
        "                node_counter += 1\n",
        "\n",
        "    def to_json(self, filename=\"hierarchical_tree2.json\"):\n",
        "        \"\"\"Stores hierarchical structure as JSON.\"\"\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.root.to_dict(), f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Hierarchical tree saved to {filename}\")\n",
        "\n",
        "    def save_to_sqlite(self, db_name=\"hierarchical_tree2.db\"):\n",
        "        \"\"\"Stores hierarchical structure in SQLite.\"\"\"\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS hierarchy (\n",
        "            node_id TEXT PRIMARY KEY,\n",
        "            text TEXT,\n",
        "            level INTEGER,\n",
        "            parent_id TEXT\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        def insert_node(node, parent_id=None):\n",
        "            cursor.execute(\"INSERT INTO hierarchy VALUES (?, ?, ?, ?)\", (node.node_id, node.text, node.level, parent_id))\n",
        "            for child in node.children:\n",
        "                insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Hierarchical tree stored in {db_name}\")\n",
        "\n",
        "    def save_to_mongodb(self, mongo_db=\"textbook2_db\", collection_name=\"hierarchical_tree2\"):\n",
        "        \"\"\"Stores hierarchical structure in MongoDB.\"\"\"\n",
        "        client = MongoClient(\"mongodb+srv://anant22067:db6aM9UfKxitrBw0@cluster0.oyo8k.mongodb.net/\")\n",
        "        db = client[mongo_db]\n",
        "        collection = db[collection_name]\n",
        "\n",
        "        collection.delete_many({})  # Clear previous data\n",
        "        def insert_node(node, parent_id=None):\n",
        "                \"\"\"Recursively insert nodes into MongoDB as separate documents.\"\"\"\n",
        "                document = {\n",
        "                    \"node_id\": node.node_id,\n",
        "                    \"text\": node.text,\n",
        "                    \"level\": node.level,\n",
        "                    \"parent_id\": parent_id  # Store parent reference\n",
        "                }\n",
        "                collection.insert_one(document)\n",
        "\n",
        "                for child in node.children:\n",
        "                    insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)  # Start inserting from the root node\n",
        "\n",
        "        print(f\"Hierarchical tree saved to MongoDB (DB: {mongo_db}, Collection: {collection_name})\")\n",
        "\n",
        "# Usage Example\n",
        "if __name__ == \"__main__\":\n",
        "    # Load extracted data from JSON\n",
        "    with open(\"extracted_medical_text2.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        extracted_data = json.load(f)\n",
        "\n",
        "    # Build hierarchical tree\n",
        "    tree = HierarchicalTree()\n",
        "    tree.parse_structure(extracted_data)\n",
        "\n",
        "    # Save to different storage formats\n",
        "    tree.to_json()  # Save as JSON\n",
        "    tree.save_to_sqlite()  # Save to SQLite\n",
        "    tree.save_to_mongodb()  # Save to MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hierarchical tree saved to hierarchical_tree3.json\n",
            "Hierarchical tree stored in hierarchical_tree3.db\n"
          ]
        },
        {
          "ename": "AutoReconnect",
          "evalue": "SSL handshake failed: cluster0-shard-00-02.oyo8k.mongodb.net:27017: [WinError 10054] An existing connection was forcibly closed by the remote host (configured timeouts: connectTimeoutMS: 20000.0ms)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:875\u001b[0m, in \u001b[0;36m_configured_socket\u001b[1;34m(address, options)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_SYNC:\n\u001b[1;32m--> 875\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mAutoReconnect\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m tree\u001b[38;5;241m.\u001b[39mto_json()  \u001b[38;5;66;03m# Save as JSON\u001b[39;00m\n\u001b[0;32m    146\u001b[0m tree\u001b[38;5;241m.\u001b[39msave_to_sqlite()  \u001b[38;5;66;03m# Save to SQLite\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_mongodb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Save to MongoDB\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[1], line 130\u001b[0m, in \u001b[0;36mHierarchicalTree.save_to_mongodb\u001b[1;34m(self, mongo_db, collection_name)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m    128\u001b[0m             insert_node(child, node\u001b[38;5;241m.\u001b[39mnode_id)\n\u001b[1;32m--> 130\u001b[0m \u001b[43minsert_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start inserting from the root node\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHierarchical tree saved to MongoDB (DB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmongo_db\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Collection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[1], line 128\u001b[0m, in \u001b[0;36mHierarchicalTree.save_to_mongodb.<locals>.insert_node\u001b[1;34m(node, parent_id)\u001b[0m\n\u001b[0;32m    125\u001b[0m collection\u001b[38;5;241m.\u001b[39mupdate_one({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: node\u001b[38;5;241m.\u001b[39mnode_id}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$set\u001b[39m\u001b[38;5;124m\"\u001b[39m: document}, upsert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43minsert_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1], line 128\u001b[0m, in \u001b[0;36mHierarchicalTree.save_to_mongodb.<locals>.insert_node\u001b[1;34m(node, parent_id)\u001b[0m\n\u001b[0;32m    125\u001b[0m collection\u001b[38;5;241m.\u001b[39mupdate_one({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: node\u001b[38;5;241m.\u001b[39mnode_id}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$set\u001b[39m\u001b[38;5;124m\"\u001b[39m: document}, upsert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43minsert_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1], line 125\u001b[0m, in \u001b[0;36mHierarchicalTree.save_to_mongodb.<locals>.insert_node\u001b[1;34m(node, parent_id)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recursively insert nodes into MongoDB as separate documents.\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m document \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: node\u001b[38;5;241m.\u001b[39mnode_id,\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: node\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: node\u001b[38;5;241m.\u001b[39mlevel,\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: parent_id  \u001b[38;5;66;03m# Store parent reference\u001b[39;00m\n\u001b[0;32m    124\u001b[0m }\n\u001b[1;32m--> 125\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m    128\u001b[0m     insert_node(child, node\u001b[38;5;241m.\u001b[39mnode_id)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\collection.py:1336\u001b[0m, in \u001b[0;36mCollection.update_one\u001b[1;34m(self, filter, update, upsert, bypass_document_validation, collation, array_filters, hint, session, let, sort, comment)\u001b[0m\n\u001b[0;32m   1332\u001b[0m common\u001b[38;5;241m.\u001b[39mvalidate_list_or_none(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_filters\u001b[39m\u001b[38;5;124m\"\u001b[39m, array_filters)\n\u001b[0;32m   1334\u001b[0m write_concern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_concern_for(session)\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UpdateResult(\n\u001b[1;32m-> 1336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_retryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_Op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUPDATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbypass_doc_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_document_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1351\u001b[0m     write_concern\u001b[38;5;241m.\u001b[39macknowledged,\n\u001b[0;32m   1352\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\collection.py:1118\u001b[0m, in \u001b[0;36mCollection._update_retryable\u001b[1;34m(self, criteria, document, operation, upsert, multi, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, hint, session, let, sort, comment)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\n\u001b[0;32m   1096\u001b[0m     session: Optional[ClientSession], conn: Connection, retryable_write: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m   1097\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(\n\u001b[0;32m   1099\u001b[0m         conn,\n\u001b[0;32m   1100\u001b[0m         criteria,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m   1116\u001b[0m     )\n\u001b[1;32m-> 1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macknowledged\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmulti\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:1896\u001b[0m, in \u001b[0;36mMongoClient._retryable_write\u001b[1;34m(self, retryable, func, session, operation, bulk, operation_id)\u001b[0m\n\u001b[0;32m   1882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute an operation with consecutive retries if possible\u001b[39;00m\n\u001b[0;32m   1883\u001b[0m \n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03mReturns func()'s return value on success. On error retries the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;124;03m:param bulk: bulk abstraction to execute operations in bulk, defaults to None\u001b[39;00m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_with_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:1782\u001b[0m, in \u001b[0;36mMongoClient._retry_with_session\u001b[1;34m(self, retryable, func, session, bulk, operation, operation_id)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;66;03m# Ensure that the options supports retry_writes and there is a valid session not in\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this txn.\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[0;32m   1780\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_writes \u001b[38;5;129;01mand\u001b[39;00m session \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction\n\u001b[0;32m   1781\u001b[0m )\n\u001b[1;32m-> 1782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\_csot.py:119\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:1828\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[1;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1802\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1803\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m \n\u001b[0;32m   1806\u001b[0m \u001b[38;5;124;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;124;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:2565\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[0;32m   2569\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:2674\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2672\u001b[0m is_mongos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_server()\n\u001b[1;32m-> 2674\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wire_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_wire_version\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msessions_supported\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2677\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\n\u001b[0;32m   2678\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretryable_writes_supported\u001b[49m\n\u001b[0;32m   2679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupports_sessions\u001b[49m\n\u001b[0;32m   2680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\mongo_client.py:1591\u001b[0m, in \u001b[0;36mMongoClient._checkout\u001b[1;34m(self, server, session)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_pinned_connection\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1591\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_handler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pin this session to the selected server or connection.\u001b[39;49;00m\n\u001b[0;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_txn\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:1346\u001b[0m, in \u001b[0;36mPool.checkout\u001b[1;34m(self, handler)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled_for_logging \u001b[38;5;129;01mand\u001b[39;00m _CONNECTION_LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n\u001b[0;32m   1338\u001b[0m     _debug_log(\n\u001b[0;32m   1339\u001b[0m         _CONNECTION_LOGGER,\n\u001b[0;32m   1340\u001b[0m         clientId\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         serverPort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1344\u001b[0m     )\n\u001b[1;32m-> 1346\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckout_started_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m checkout_started_time\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled_for_cmap:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:1507\u001b[0m, in \u001b[0;36mPool._get_conn\u001b[1;34m(self, checkout_started_time, handler)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# We need to create a new connection\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1507\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_connecting_cond:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:1265\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self, handler)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     _debug_log(\n\u001b[0;32m   1256\u001b[0m         _CONNECTION_LOGGER,\n\u001b[0;32m   1257\u001b[0m         clientId\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         driverConnectionId\u001b[38;5;241m=\u001b[39mconn_id,\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1265\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43m_configured_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:905\u001b[0m, in \u001b[0;36m_configured_socket\u001b[1;34m(address, options)\u001b[0m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;66;03m# We raise AutoReconnect for transient and permanent SSL handshake\u001b[39;00m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;66;03m# failures alike. Permanent handshake failures, like protocol\u001b[39;00m\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;66;03m# mismatch, will be turned into ServerSelectionTimeoutErrors later.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     details \u001b[38;5;241m=\u001b[39m _get_timeout_details(options)\n\u001b[1;32m--> 905\u001b[0m     \u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSL handshake failed: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    907\u001b[0m     ssl_context\u001b[38;5;241m.\u001b[39mverify_mode\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mcheck_hostname\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39mtls_allow_invalid_hostnames\n\u001b[0;32m    910\u001b[0m ):\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymongo\\synchronous\\pool.py:211\u001b[0m, in \u001b[0;36m_raise_connection_failure\u001b[1;34m(address, error, msg_prefix, timeout_details)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NetworkTimeout(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
            "\u001b[1;31mAutoReconnect\u001b[0m: SSL handshake failed: cluster0-shard-00-02.oyo8k.mongodb.net:27017: [WinError 10054] An existing connection was forcibly closed by the remote host (configured timeouts: connectTimeoutMS: 20000.0ms)"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "from pymongo import MongoClient\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Represents a node in the hierarchical tree.\"\"\"\n",
        "    def __init__(self, node_id, text, level, parent=None):\n",
        "        self.node_id = node_id\n",
        "        self.text = text\n",
        "        self.level = level  # 0: root, 1: chapter, 2: section, 3: subsection, 4: paragraph\n",
        "        self.children = []\n",
        "        self.parent = parent\n",
        "\n",
        "    def add_child(self, child):\n",
        "        \"\"\"Adds a child node.\"\"\"\n",
        "        self.children.append(child)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Converts node to dictionary format for JSON storage.\"\"\"\n",
        "        return {\n",
        "            \"node_id\": self.node_id,\n",
        "            \"text\": self.text,\n",
        "            \"level\": self.level,\n",
        "            \"children\": [child.to_dict() for child in self.children]\n",
        "        }\n",
        "\n",
        "class HierarchicalTree:\n",
        "    \"\"\"Creates and manages the hierarchical structure of the textbook.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.root = Node(\"root\", \"Textbook\", 0)\n",
        "\n",
        "    def parse_structure(self, extracted_data):\n",
        "        \"\"\"Parses structured text and organizes it into a hierarchical tree.\"\"\"\n",
        "        current_chapter = None\n",
        "        current_section = None\n",
        "        current_subsection = None\n",
        "        node_counter = 1  # Unique identifier for nodes\n",
        "\n",
        "        for item in extracted_data[\"text\"]:\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            # Identify structure based on patterns\n",
        "            if re.match(r\"^\\d+\\.\\s+[A-Z]\", text):  # Chapter (e.g., \"1. Introduction\")\n",
        "                current_chapter = Node(f\"ch_{node_counter}\", text, 1, self.root)\n",
        "                self.root.add_child(current_chapter)\n",
        "                current_section = None\n",
        "                current_subsection = None\n",
        "                node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\s+[A-Z]\", text):  # Section (e.g., \"1.1 Background\")\n",
        "                if current_chapter:\n",
        "                    current_section = Node(f\"sec_{node_counter}\", text, 2, current_chapter)\n",
        "                    current_chapter.add_child(current_section)\n",
        "                    current_subsection = None\n",
        "                    node_counter += 1\n",
        "\n",
        "            elif re.match(r\"^\\d+\\.\\d+\\.\\d+\\s+[A-Z]\", text):  # Subsection (e.g., \"1.1.1 Definition\")\n",
        "                if current_section:\n",
        "                    current_subsection = Node(f\"subsec_{node_counter}\", text, 3, current_section)\n",
        "                    current_section.add_child(current_subsection)\n",
        "                    node_counter += 1\n",
        "\n",
        "            else:  # Paragraphs\n",
        "                if current_subsection:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_subsection)\n",
        "                    current_subsection.add_child(paragraph_node)\n",
        "                elif current_section:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_section)\n",
        "                    current_section.add_child(paragraph_node)\n",
        "                elif current_chapter:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, current_chapter)\n",
        "                    current_chapter.add_child(paragraph_node)\n",
        "                else:\n",
        "                    paragraph_node = Node(f\"para_{node_counter}\", text, 4, self.root)\n",
        "                    self.root.add_child(paragraph_node)\n",
        "\n",
        "                node_counter += 1\n",
        "\n",
        "    def to_json(self, filename=\"hierarchical_tree3.json\"):\n",
        "        \"\"\"Stores hierarchical structure as JSON.\"\"\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.root.to_dict(), f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Hierarchical tree saved to {filename}\")\n",
        "\n",
        "    def save_to_sqlite(self, db_name=\"hierarchical_tree3.db\"):\n",
        "        \"\"\"Stores hierarchical structure in SQLite.\"\"\"\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS hierarchy (\n",
        "            node_id TEXT PRIMARY KEY,\n",
        "            text TEXT,\n",
        "            level INTEGER,\n",
        "            parent_id TEXT\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        def insert_node(node, parent_id=None):\n",
        "            cursor.execute(\"INSERT INTO hierarchy VALUES (?, ?, ?, ?)\", (node.node_id, node.text, node.level, parent_id))\n",
        "            for child in node.children:\n",
        "                insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Hierarchical tree stored in {db_name}\")\n",
        "\n",
        "    def save_to_mongodb(self, mongo_db=\"textbook3_db\", collection_name=\"hierarchical_tree3\"):\n",
        "        \"\"\"Stores hierarchical structure in MongoDB.\"\"\"\n",
        "        client = MongoClient(\"mongodb+srv://anant22067:db6aM9UfKxitrBw0@cluster0.oyo8k.mongodb.net/\")\n",
        "        db = client[mongo_db]\n",
        "        collection = db[collection_name]\n",
        "\n",
        "        collection.delete_many({})  # Clear previous data\n",
        "        def insert_node(node, parent_id=None):\n",
        "                \"\"\"Recursively insert nodes into MongoDB as separate documents.\"\"\"\n",
        "                document = {\n",
        "                    \"node_id\": node.node_id,\n",
        "                    \"text\": node.text,\n",
        "                    \"level\": node.level,\n",
        "                    \"parent_id\": parent_id  # Store parent reference\n",
        "                }\n",
        "                collection.update_one({\"node_id\": node.node_id}, {\"$set\": document}, upsert=True)\n",
        "\n",
        "                for child in node.children:\n",
        "                    insert_node(child, node.node_id)\n",
        "\n",
        "        insert_node(self.root)  # Start inserting from the root node\n",
        "\n",
        "        print(f\"Hierarchical tree saved to MongoDB (DB: {mongo_db}, Collection: {collection_name})\")\n",
        "\n",
        "# Usage Example\n",
        "if __name__ == \"__main__\":\n",
        "    # Load extracted data from JSON\n",
        "    with open(\"extracted_medical_text3.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        extracted_data = json.load(f)\n",
        "\n",
        "    # Build hierarchical tree\n",
        "    tree = HierarchicalTree()\n",
        "    tree.parse_structure(extracted_data)\n",
        "\n",
        "    # Save to different storage formats\n",
        "    tree.to_json()  # Save as JSON\n",
        "    tree.save_to_sqlite()  # Save to SQLite\n",
        "    tree.save_to_mongodb()  # Save to MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer.six in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (20231228)\n",
            "Requirement already satisfied: pymupdf in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.25.2)\n",
            "Requirement already satisfied: pdfplumber in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.11.5)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfminer.six) (44.0.0)\n",
            "Requirement already satisfied: Pillow>=9.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pdfminer.six pymupdf pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.11)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pymongo) (2.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rank_bm25) (2.1.3)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.5 MB 660.6 kB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.2/1.5 MB 2.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.5/1.5 MB 10.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensimNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~cipy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~cipy'.\n",
            "  You can safely remove it manually.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
            "     ------ --------------------------------- 10.2/60.6 kB ? eta -:--:--\n",
            "     ------------------------- ------------ 41.0/60.6 kB 653.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 60.6/60.6 kB 645.4 kB/s eta 0:00:00\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
            "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 1.2/24.0 MB 26.1 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 2.8/24.0 MB 29.3 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 4.4/24.0 MB 31.4 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 6.3/24.0 MB 33.6 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 8.2/24.0 MB 34.9 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 9.9/24.0 MB 35.3 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 12.3/24.0 MB 40.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 14.6/24.0 MB 43.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 16.7/24.0 MB 43.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 19.7/24.0 MB 50.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 21.6/24.0 MB 50.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.6/24.0 MB 46.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 46.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.0/24.0 MB 1.3 MB/s eta 0:00:00\n",
            "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
            "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
            "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 1.8/46.2 MB 39.0 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 3.7/46.2 MB 38.8 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 5.5/46.2 MB 38.9 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 7.6/46.2 MB 40.1 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 10.1/46.2 MB 28.1 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 13.1/46.2 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 15.0/46.2 MB 29.7 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 16.0/46.2 MB 31.1 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 16.0/46.2 MB 31.1 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 18.4/46.2 MB 27.3 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 18.4/46.2 MB 27.3 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 21.5/46.2 MB 28.4 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 22.2/46.2 MB 28.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 23.3/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 23.3/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 26.9/46.2 MB 29.8 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 27.5/46.2 MB 26.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 28.6/46.2 MB 25.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 31.5/46.2 MB 28.5 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 31.7/46.2 MB 24.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 31.8/46.2 MB 20.5 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 32.0/46.2 MB 20.5 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 35.4/46.2 MB 19.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 44.1/46.2 MB 21.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 46.2/46.2 MB 9.9 MB/s eta 0:00:00\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "   ---------------------------------------- 0.0/61.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 61.7/61.7 kB 3.2 MB/s eta 0:00:00\n",
            "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import torch\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\A13na\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\A13na\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pymedtermino\n",
            "  Downloading PyMedTermino-0.3.3.tar.gz (34.0 MB)\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/34.0 MB ? eta -:--:--\n",
            "     --------------------------------------- 0.1/34.0 MB 423.5 kB/s eta 0:01:20\n",
            "     - -------------------------------------- 0.8/34.0 MB 2.7 MB/s eta 0:00:13\n",
            "     - -------------------------------------- 1.0/34.0 MB 3.2 MB/s eta 0:00:11\n",
            "     - -------------------------------------- 1.0/34.0 MB 3.2 MB/s eta 0:00:11\n",
            "     - -------------------------------------- 1.0/34.0 MB 3.2 MB/s eta 0:00:11\n",
            "     - -------------------------------------- 1.0/34.0 MB 3.2 MB/s eta 0:00:11\n",
            "     - -------------------------------------- 1.0/34.0 MB 3.2 MB/s eta 0:00:11\n",
            "     - -------------------------------------- 1.3/34.0 MB 2.2 MB/s eta 0:00:16\n",
            "     -- ------------------------------------- 2.1/34.0 MB 3.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.1/34.0 MB 3.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.1/34.0 MB 3.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.1/34.0 MB 3.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.1/34.0 MB 3.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.3/34.0 MB 2.6 MB/s eta 0:00:13\n",
            "     ---- ----------------------------------- 3.4/34.0 MB 3.7 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 4.2/34.0 MB 4.4 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 4.2/34.0 MB 4.4 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 4.2/34.0 MB 4.4 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 4.2/34.0 MB 4.4 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 4.2/34.0 MB 4.4 MB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 5.0/34.0 MB 4.2 MB/s eta 0:00:07\n",
            "     ------- -------------------------------- 6.7/34.0 MB 5.4 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 8.3/34.0 MB 6.4 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 9.4/34.0 MB 7.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 9.4/34.0 MB 7.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 9.4/34.0 MB 7.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 9.4/34.0 MB 7.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 9.4/34.0 MB 7.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 10.0/34.0 MB 6.4 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 11.5/34.0 MB 9.9 MB/s eta 0:00:03\n",
            "     --------------- ----------------------- 13.5/34.0 MB 13.6 MB/s eta 0:00:02\n",
            "     ----------------- --------------------- 15.3/34.0 MB 19.9 MB/s eta 0:00:01\n",
            "     ------------------- ------------------- 17.3/34.0 MB 20.5 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 17.8/34.0 MB 21.1 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 17.8/34.0 MB 21.1 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 17.8/34.0 MB 21.1 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 17.8/34.0 MB 21.1 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 17.9/34.0 MB 14.6 MB/s eta 0:00:02\n",
            "     ---------------------- ---------------- 19.6/34.0 MB 14.2 MB/s eta 0:00:02\n",
            "     ------------------------ -------------- 21.7/34.0 MB 22.6 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 22.0/34.0 MB 21.9 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 22.0/34.0 MB 21.9 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 22.0/34.0 MB 21.9 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 22.0/34.0 MB 21.9 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 22.3/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 24.1/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 24.1/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 24.1/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 24.1/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 24.1/34.0 MB 15.2 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 25.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 27.6/34.0 MB 11.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.2/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.4/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.4/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.4/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.4/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 30.4/34.0 MB 16.0 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 31.5/34.0 MB 11.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 31.5/34.0 MB 11.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 31.5/34.0 MB 11.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 31.5/34.0 MB 11.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 32.1/34.0 MB 9.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  34.0/34.0 MB 11.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 34.0/34.0 MB 2.4 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl (13.7 MB)\n",
            "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/13.7 MB 5.4 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 2.3/13.7 MB 24.7 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 3.5/13.7 MB 24.8 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 5.8/13.7 MB 30.8 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 8.0/13.7 MB 34.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 10.2/13.7 MB 36.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 11.6/13.7 MB 38.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.4/13.7 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.7/13.7 MB 43.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.7/13.7 MB 43.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.7/13.7 MB 28.5 MB/s eta 0:00:00\n",
            "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "   ---------------------------------------- 0.0/468.8 kB ? eta -:--:--\n",
            "   --------------------------------------- 468.8/468.8 kB 28.7 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: pymedtermino\n",
            "  Building wheel for pymedtermino (pyproject.toml): started\n",
            "  Building wheel for pymedtermino (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for pymedtermino: filename=PyMedTermino-0.3.3-py3-none-any.whl size=34423170 sha256=e4c6285548a435b83ef996d21837b3e9f0da8e34352565137ec4b3e582725f9e\n",
            "  Stored in directory: c:\\users\\a13na\\appdata\\local\\pip\\cache\\wheels\\94\\ed\\fe\\df89cd582f8091826e36cecc7cf7fe18c9b92c77d2a872216c\n",
            "Successfully built pymedtermino\n",
            "Installing collected packages: whoosh, pymedtermino, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0 pymedtermino-0.3.3 whoosh-2.7.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu whoosh pymedtermino "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "[WinError 5] Access is denied: 'bm25_index\\\\MAIN.tmp'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    129\u001b[0m     text_data \u001b[38;5;241m=\u001b[39m load_text_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhierarchical_tree2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mbuild_bm25_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     build_dpr_index(text_data)\n\u001b[0;32m    133\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheart attack treatment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[1;32mIn[16], line 75\u001b[0m, in \u001b[0;36mbuild_bm25_index\u001b[1;34m(text_data, index_dir)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m  item \u001b[38;5;129;01min\u001b[39;00m text_data:\n\u001b[0;32m     74\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_document(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], content\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 75\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whoosh\\writing.py:935\u001b[0m, in \u001b[0;36mSegmentWriter.commit\u001b[1;34m(self, mergetype, optimize, merge)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_commit_toc(finalsegments)\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# Final cleanup\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whoosh\\writing.py:884\u001b[0m, in \u001b[0;36mSegmentWriter._finish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_finish\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 884\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tempstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwritelock:\n\u001b[0;32m    886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwritelock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whoosh\\filedb\\filestore.py:467\u001b[0m, in \u001b[0;36mFileStorage.destroy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whoosh\\filedb\\filestore.py:461\u001b[0m, in \u001b[0;36mFileStorage.destroy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean()\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try to remove the directory\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     e \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'bm25_index\\\\MAIN.tmp'"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import faiss\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.qparser import QueryParser\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "from transformers import DPRQuestionEncoderTokenizer, DPRQuestionEncoder\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "# Load Sentence-BERT & Cross-Encoder\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# Load SNOMED-CT and MeSH datasets\n",
        "snomed_df = pd.read_csv(\"SNOMED-CT_cleaned.csv\")  # Ensure this file is available\n",
        "mesh_df = pd.read_csv(\"MeSH_terms.csv\")  # Ensure this file is available\n",
        "\n",
        "def load_text_data(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    def extract_text(node):\n",
        "        \"\"\" Recursively extract text from hierarchical tree \"\"\"\n",
        "        texts = [{\"id\": node[\"node_id\"], \"text\": node[\"text\"]}]\n",
        "        for child in node.get(\"children\", []):\n",
        "            texts.extend(extract_text(child))  # Recursively process children\n",
        "        return texts\n",
        "\n",
        "    return extract_text(data)  # Start extraction from the root node\n",
        "\n",
        "# Query Expansion using SNOMED-CT, MeSH, and WordNet\n",
        "def expand_query(query):\n",
        "    expanded_terms = set()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for word in query.split():\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        stem = stemmer.stem(word)\n",
        "        expanded_terms.update([word, lemma, stem])\n",
        "\n",
        "        # Add WordNet synonyms\n",
        "        syns = wordnet.synsets(word)\n",
        "        for syn in syns:\n",
        "            for lemma in syn.lemmas():\n",
        "                expanded_terms.add(lemma.name())\n",
        "\n",
        "        # Add SNOMED-CT synonyms\n",
        "        snomed_synonyms = snomed_df[snomed_df['Concept'] == word]['Synonyms'].values\n",
        "        if len(snomed_synonyms) > 0:\n",
        "            expanded_terms.update(snomed_synonyms[0].split(','))\n",
        "\n",
        "        # Add MeSH synonyms\n",
        "        mesh_synonyms = mesh_df[mesh_df['Term'] == word]['Synonyms'].values\n",
        "        if len(mesh_synonyms) > 0:\n",
        "            expanded_terms.update(mesh_synonyms[0].split(','))\n",
        "\n",
        "    return \" \".join(expanded_terms)\n",
        "\n",
        "# BM25 using Whoosh\n",
        "def build_bm25_index(text_data, index_dir=\"bm25_index\"):\n",
        "    if not os.path.exists(index_dir):\n",
        "        os.mkdir(index_dir)\n",
        "        schema = Schema(id=ID(stored=True), content=TEXT)\n",
        "        ix = create_in(index_dir, schema)\n",
        "        writer = ix.writer()\n",
        "        for  item in text_data:\n",
        "            writer.add_document(id=item[\"id\"], content=item[\"text\"])\n",
        "        writer.commit()\n",
        "\n",
        "def bm25_search(query, top_k=10, index_dir=\"bm25_index\"):\n",
        "    ix = open_dir(index_dir)\n",
        "    qp = QueryParser(\"content\", ix.schema)\n",
        "    with ix.searcher() as searcher:\n",
        "        results = searcher.search(qp.parse(query), limit=top_k)\n",
        "        return [(hit[\"id\"], hit.score) for hit in results]\n",
        "\n",
        "# Dense Passage Retrieval (DPR) Indexing with FAISS\n",
        "def build_dpr_index(text_data, index_path=\"dpr_index.faiss\"):\n",
        "    encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "    tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    for item in text_data:\n",
        "        input_ids = tokenizer(item[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        embedding = encoder(**input_ids).pooler_output.detach().numpy()\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "def dpr_search(query, top_k=10, index_path=\"dpr_index.faiss\"):\n",
        "    index = faiss.read_index(index_path)\n",
        "    encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "    tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    input_ids = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    query_embedding = encoder(**input_ids).pooler_output.detach().numpy()\n",
        "    _, idxs = index.search(query_embedding, top_k)\n",
        "    return list(idxs[0])\n",
        "\n",
        "# Cross-Encoder Re-Ranking\n",
        "def rerank_results(query, results, text_data):\n",
        "    pairs = [(query, text_data[int(doc_id)][\"text\"]) for doc_id, _ in results]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    reranked_results = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)\n",
        "    return reranked_results\n",
        "\n",
        "# Main Retrieval Pipeline\n",
        "def retrieve(query, text_data, top_k=10):\n",
        "    expanded_query = expand_query(query)\n",
        "\n",
        "    bm25_results = bm25_search(expanded_query, top_k)\n",
        "    dpr_results = [(doc_id, 1.0) for doc_id in dpr_search(query, top_k)]\n",
        "\n",
        "    combined_results = list(set(bm25_results + dpr_results))\n",
        "    reranked_results = rerank_results(query, combined_results, text_data)\n",
        "    return reranked_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text_data = load_text_data(\"hierarchical_tree2.json\")\n",
        "    build_bm25_index(text_data)\n",
        "    build_dpr_index(text_data)\n",
        "\n",
        "    query = \"heart attack treatment\"\n",
        "    results = retrieve(query, text_data, top_k=10)\n",
        "    for rank, ((doc_id, score), re_score) in enumerate(results):\n",
        "        print(f\"{rank+1}. Doc {doc_id} - Score: {re_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flattened text data: [{'id': 'root', 'content': 'Textbook'}, {'id': 'para_1', 'content': 'Goodman & Gilman’s'}, {'id': 'para_2', 'content': 'The'}, {'id': 'para_3', 'content': 'Pharmacological'}, {'id': 'para_4', 'content': 'Basis of'}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def flatten_json(node, results=None):\n",
        "    \"\"\"\n",
        "    Recursively flatten a hierarchical JSON structure to extract `text`.\n",
        "    :param node: Current node in the hierarchy.\n",
        "    :param results: List to store the flattened results.\n",
        "    :return: Flattened list of dictionaries with `id` and `content`.\n",
        "    \"\"\"\n",
        "    if results is None:\n",
        "        results = []\n",
        "\n",
        "    # Add the current node's text to the results\n",
        "    if \"text\" in node and \"node_id\" in node:\n",
        "        results.append({\"id\": node[\"node_id\"], \"content\": node[\"text\"]})\n",
        "\n",
        "    # Recursively process children\n",
        "    for child in node.get(\"children\", []):\n",
        "        flatten_json(child, results)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"hierarchical_tree.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Flatten the hierarchy\n",
        "text_data = flatten_json(data)\n",
        "print(f\"Flattened text data: {text_data[:5]}\")  # Print first 5 entries for verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Building DPR Index:   0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Building DPR Index: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index saved to dpr_index_test.faiss\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "def build_dpr_index(text_data, index_path=\"dpr_index.faiss\", batch_size=128):\n",
        "    \"\"\"Build the FAISS index for DPR.\"\"\"\n",
        "    encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "    tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    index = faiss.IndexFlatL2(768)  # 768-dimensional embeddings\n",
        "    for i in tqdm(range(0, len(text_data), batch_size), desc=\"Building DPR Index\"):\n",
        "        batch = text_data[i:i + batch_size]\n",
        "        batch_texts = [item[\"content\"] for item in batch]  # Extract 'content' field from JSON\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            embeddings = encoder(**inputs).pooler_output.numpy()\n",
        "        index.add(embeddings)\n",
        "\n",
        "    # Save the FAISS index\n",
        "    faiss.write_index(index, index_path)\n",
        "    print(f\"FAISS index saved to {index_path}\")\n",
        "\n",
        "build_dpr_index(text_data[:50], index_path=\"dpr_index_test.faiss\")  # Use only the first 50 entries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_document_retrieval(query, bm25_top_k=5, dpr_top_k=5):\n",
        "    # BM25 Retrieval\n",
        "    bm25_results = bm25_search(query, top_k=bm25_top_k)\n",
        "\n",
        "    # DPR Retrieval\n",
        "    dpr_results = dpr_search(query, top_k=dpr_top_k, index_path=\"dpr_index.faiss\")\n",
        "\n",
        "    # Combine results (e.g., union of BM25 and DPR with scores)\n",
        "    combined_results = {result[\"id\"]: result for result in bm25_results + dpr_results}\n",
        "    return list(combined_results.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hierarchical_traversal(node, query, results=None):\n",
        "    \"\"\"\n",
        "    Traverse the hierarchy and extract relevant nodes based on the query.\n",
        "    \"\"\"\n",
        "    if results is None:\n",
        "        results = []\n",
        "\n",
        "    # Check if the current node is relevant (basic matching or scoring)\n",
        "    if query.lower() in node.get(\"text\", \"\").lower():\n",
        "        results.append({\"id\": node[\"node_id\"], \"content\": node[\"text\"]})\n",
        "\n",
        "    # Traverse children recursively\n",
        "    for child in node.get(\"children\", []):\n",
        "        hierarchical_traversal(child, query, results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traverse the JSON hierarchy for the query\n",
        "query_results = hierarchical_traversal(data, query=\"pharmacological\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from whoosh.index import open_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.index import open_dir\n",
        "\n",
        "def bm25_search(query, top_k=10, index_dir=\"bm25_index2\"):\n",
        "    \"\"\"\n",
        "    Search the BM25 index for the top_k relevant documents.\n",
        "    \"\"\"\n",
        "    ix = open_dir(index_dir)  # Open the BM25 index directory\n",
        "    qp = QueryParser(\"content\", ix.schema)  # Query parser for the \"content\" field\n",
        "    with ix.searcher() as searcher:\n",
        "        results = searcher.search(qp.parse(query), limit=top_k)\n",
        "        return [{\"id\": hit[\"id\"], \"content\": hit[\"content\"], \"score\": hit.score} for hit in results]\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_document_retrieval(query, bm25_top_k=5, dpr_top_k=5):\n",
        "    bm25_results = bm25_search(query, top_k=bm25_top_k)\n",
        "    dpr_results = dpr_search(query, top_k=dpr_top_k, index_path=\"dpr_index_test.faiss\")\n",
        "\n",
        "    combined_results = {result[\"id\"]: result for result in (bm25_results + dpr_results)}\n",
        "\n",
        "    # Debug: Print results to ensure no nested structures\n",
        "    print(\"Combined Results:\", combined_results)\n",
        "    return list(combined_results.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_context(retrieved_content):\n",
        "    \"\"\"\n",
        "    Extract and combine the content from the retrieved documents into a single context string.\n",
        "    Ensures the content field is properly handled as a string.\n",
        "    \"\"\"\n",
        "    if not retrieved_content:\n",
        "        print(\"No retrieved content found. Returning an empty context.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Extract content from retrieved results and join into a single string\n",
        "    try:\n",
        "        context = \" \".join([\n",
        "            item[\"content\"] if isinstance(item[\"content\"], str) else str(item[\"content\"])\n",
        "            for item in retrieved_content\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        print(f\"Error during context extraction: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Results: {5: {'id': 5, 'content': {'id': 'para_5', 'content': 'THERAPEUTICS'}}, 27: {'id': 27, 'content': {'id': 'para_27', 'content': 'THERAPEUTICS'}}, 3: {'id': 3, 'content': {'id': 'para_3', 'content': 'Pharmacological'}}, 25: {'id': 25, 'content': {'id': 'para_25', 'content': 'Pharmacological'}}, 20: {'id': 20, 'content': {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'}}}\n",
            "Context extracted successfully: {'id': 'para_5', 'content': 'THERAPEUTICS'} {'id': 'para_27', 'content': 'THERAPEUTICS'} {'id': 'para_3', 'content': 'Pharmacological'} {'id': 'para_25', 'content': 'Pharmacological'} {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'}\n",
            "Answer: THERAPEUTICS\n",
            "Evidence: [{'id': 5, 'content': {'id': 'para_5', 'content': 'THERAPEUTICS'}}, {'id': 27, 'content': {'id': 'para_27', 'content': 'THERAPEUTICS'}}, {'id': 3, 'content': {'id': 'para_3', 'content': 'Pharmacological'}}, {'id': 25, 'content': {'id': 'para_25', 'content': 'Pharmacological'}}, {'id': 20, 'content': {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'}}]\n"
          ]
        }
      ],
      "source": [
        "response = full_rag_pipeline(query=\"pharmacological treatments\")\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "print(\"Evidence:\", response[\"evidence\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\A13na\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "def generate_answer_with_flan_t5(query, context=\"\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using Flan-T5.\n",
        "    If context is provided, it will use it to generate a more accurate response.\n",
        "    \"\"\"\n",
        "    if context:\n",
        "        prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    else:\n",
        "        prompt = f\"Answer the following question:\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    \n",
        "    try:\n",
        "        # Generate the answer\n",
        "        response = llm(prompt, max_length=200, truncation=True)\n",
        "        return response[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Flan-T5 Error: {e}\")\n",
        "        return \"Sorry, I couldn't generate an answer at this time.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "import faiss\n",
        "\n",
        "def dpr_search(query, top_k=10, index_path=\"dpr_index_test.faiss\"):\n",
        "    # Load the FAISS index\n",
        "    index = faiss.read_index(index_path)\n",
        "\n",
        "    # Load the DPR Question Encoder and Tokenizer\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    # Encode the user query\n",
        "    inputs = question_tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output.numpy()\n",
        "\n",
        "    # Search the FAISS index\n",
        "    if index.ntotal == 0:\n",
        "        print(\"FAISS index is empty.\")\n",
        "        return []\n",
        "\n",
        "    distances, idxs = index.search(query_embedding, top_k)\n",
        "    if len(idxs[0]) == 0:\n",
        "        print(\"No results found in FAISS index.\")\n",
        "        return []\n",
        "\n",
        "    # Return the document IDs from the FAISS index\n",
        "    return [{\"id\": idx, \"content\": text_data[idx]} for idx in idxs[0] if idx < len(text_data)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_context(retrieved_content):\n",
        "    \"\"\"\n",
        "    Extract and combine the content from the retrieved documents into a single context string.\n",
        "    Ensures the content field is properly handled as a string.\n",
        "    \"\"\"\n",
        "    if not retrieved_content:\n",
        "        print(\"No retrieved content found. Returning an empty context.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Safely extract content and handle potential nested dictionaries\n",
        "    context = \" \".join([\n",
        "        item[\"content\"] if isinstance(item[\"content\"], str) else str(item[\"content\"])\n",
        "        for item in retrieved_content\n",
        "    ])\n",
        "\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\A13na\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "def generate_answer_with_flan_t5(query, context=\"\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using Flan-T5.\n",
        "    If context is provided, it will use it to generate a more accurate response.\n",
        "    \"\"\"\n",
        "    if context:\n",
        "        prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    else:\n",
        "        prompt = f\"Answer the following question:\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    \n",
        "    try:\n",
        "        # Generate the answer\n",
        "        response = llm(prompt, max_length=200, truncation=True)\n",
        "        return response[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Flan-T5 Error: {e}\")\n",
        "        return \"Sorry, I couldn't generate an answer at this time.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def full_rag_pipeline(query):\n",
        "    # Step 1: Retrieve relevant content\n",
        "    retrieved_content = multi_document_retrieval(query)\n",
        "\n",
        "    if not retrieved_content:  # No relevant results found\n",
        "        print(\"No results found in BM25 or DPR. Falling back to Flan-T5.\")\n",
        "        return {\n",
        "            \"answer\": generate_answer_with_flan_t5(query=query, context=\"\"),\n",
        "            \"evidence\": []\n",
        "        }\n",
        "\n",
        "    # Step 2: Extract context\n",
        "    try:\n",
        "        context = extract_context(retrieved_content)\n",
        "        print(\"Context extracted successfully:\", context)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during context extraction: {e}\")\n",
        "        return {\n",
        "            \"answer\": \"Error: Could not extract context.\",\n",
        "            \"evidence\": retrieved_content\n",
        "        }\n",
        "\n",
        "    # Step 3: Generate answer using the extracted context\n",
        "    answer = generate_answer_with_flan_t5(query=query, context=context)\n",
        "\n",
        "    return {\"answer\": answer, \"evidence\": retrieved_content}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Results: {32: {'id': 32, 'content': {'id': 'para_32', 'content': 'San Juan   Seoul   Singapore   Sydney   Toronto'}}, 31: {'id': 31, 'content': {'id': 'para_31', 'content': 'New York   Chicago   San Francisco   Lisbon  London   Madrid   Mexico City   Milan   New Delhi'}}, 39: {'id': 39, 'content': {'id': 'para_39', 'content': 'names  in  an  editorial  fashion  only,  and  to  the  benefit  of  the  trademark  owner,  with  no  intention  of  infringement  of  the  trademark.  Where  such'}}, 40: {'id': 40, 'content': {'id': 'para_40', 'content': 'designations appear in this book, they have been printed with initial caps.'}}, 49: {'id': 49, 'content': {'id': 'para_49', 'content': 'THE WORK IS PROVIDED “AS IS.” McGRAW-HILL AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS TO THE ACCU-'}}}\n",
            "Context extracted successfully: {'id': 'para_32', 'content': 'San Juan   Seoul   Singapore   Sydney   Toronto'} {'id': 'para_31', 'content': 'New York   Chicago   San Francisco   Lisbon  London   Madrid   Mexico City   Milan   New Delhi'} {'id': 'para_39', 'content': 'names  in  an  editorial  fashion  only,  and  to  the  benefit  of  the  trademark  owner,  with  no  intention  of  infringement  of  the  trademark.  Where  such'} {'id': 'para_40', 'content': 'designations appear in this book, they have been printed with initial caps.'} {'id': 'para_49', 'content': 'THE WORK IS PROVIDED “AS IS.” McGRAW-HILL AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS TO THE ACCU-'}\n",
            "Answer: New Delhi\n",
            "Evidence: [{'id': 32, 'content': {'id': 'para_32', 'content': 'San Juan   Seoul   Singapore   Sydney   Toronto'}}, {'id': 31, 'content': {'id': 'para_31', 'content': 'New York   Chicago   San Francisco   Lisbon  London   Madrid   Mexico City   Milan   New Delhi'}}, {'id': 39, 'content': {'id': 'para_39', 'content': 'names  in  an  editorial  fashion  only,  and  to  the  benefit  of  the  trademark  owner,  with  no  intention  of  infringement  of  the  trademark.  Where  such'}}, {'id': 40, 'content': {'id': 'para_40', 'content': 'designations appear in this book, they have been printed with initial caps.'}}, {'id': 49, 'content': {'id': 'para_49', 'content': 'THE WORK IS PROVIDED “AS IS.” McGRAW-HILL AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS TO THE ACCU-'}}]\n"
          ]
        }
      ],
      "source": [
        "response = full_rag_pipeline(query=\"country of New Delhi?\")\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "print(\"Evidence:\", response[\"evidence\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Results: {5: {'id': 5, 'content': {'id': 'para_5', 'content': 'THERAPEUTICS'}}, 27: {'id': 27, 'content': {'id': 'para_27', 'content': 'THERAPEUTICS'}}, 20: {'id': 20, 'content': {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'}}, 18: {'id': 18, 'content': {'id': 'para_18', 'content': 'Professor of Internal Medicine and Pharmacology'}}, 42: {'id': 42, 'content': {'id': 'para_42', 'content': 'training programs. For more information, please contact George Hoare, Special Sales, at george_hoare@mcgraw-hill.com or (212) 904-4069.'}}}\n",
            "Context extracted successfully: {'id': 'para_5', 'content': 'THERAPEUTICS'} {'id': 'para_27', 'content': 'THERAPEUTICS'} {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'} {'id': 'para_18', 'content': 'Professor of Internal Medicine and Pharmacology'} {'id': 'para_42', 'content': 'training programs. For more information, please contact George Hoare, Special Sales, at george_hoare@mcgraw-hill.com or (212) 904-4069.'}\n",
            "Answer: THERAPEUTICS\n",
            "Evidence: [{'id': 5, 'content': {'id': 'para_5', 'content': 'THERAPEUTICS'}}, {'id': 27, 'content': {'id': 'para_27', 'content': 'THERAPEUTICS'}}, {'id': 20, 'content': {'id': 'para_20', 'content': 'Chief, Division of Endocrinology and Metabolism'}}, {'id': 18, 'content': {'id': 'para_18', 'content': 'Professor of Internal Medicine and Pharmacology'}}, {'id': 42, 'content': {'id': 'para_42', 'content': 'training programs. For more information, please contact George Hoare, Special Sales, at george_hoare@mcgraw-hill.com or (212) 904-4069.'}}]\n"
          ]
        }
      ],
      "source": [
        "response = full_rag_pipeline(query=\"treatments for heart attack\")\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "print(\"Evidence:\", response[\"evidence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-scoreNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "     ---------------------------------------- 0.0/51.8 kB ? eta -:--:--\n",
            "     ------- -------------------------------- 10.2/51.8 kB ? eta -:--:--\n",
            "     ------- -------------------------------- 10.2/51.8 kB ? eta -:--:--\n",
            "     -------------------------------------  51.2/51.8 kB 435.7 kB/s eta 0:00:01\n",
            "     -------------------------------------  51.2/51.8 kB 435.7 kB/s eta 0:00:01\n",
            "     -------------------------------------  51.2/51.8 kB 435.7 kB/s eta 0:00:01\n",
            "     -------------------------------------- 51.8/51.8 kB 204.9 kB/s eta 0:00:00\n",
            "Collecting absl-py (from rouge-score)\n",
            "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rouge-score) (1.16.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (2024.11.6)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (0.4.6)\n",
            "Collecting lxml (from sacrebleu)\n",
            "  Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->rouge-score) (4.67.1)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from portalocker->sacrebleu) (308)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "   ---------------------------------------- 0.0/104.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 104.1/104.1 kB 2.9 MB/s eta 0:00:00\n",
            "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.8/3.8 MB 26.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 2.0/3.8 MB 25.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 3.3/3.8 MB 26.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.8/3.8 MB 24.4 MB/s eta 0:00:00\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (pyproject.toml): started\n",
            "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25025 sha256=51ba2450228cbe35f4779b10d1d30161d4a34dbf45c14d9d82fc8f67d3d6f120\n",
            "  Stored in directory: c:\\users\\a13na\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: tabulate, portalocker, lxml, absl-py, sacrebleu, rouge-score\n",
            "Successfully installed absl-py-2.1.0 lxml-5.3.0 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install rouge-score sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Scores: {'ROUGE-1': 0.2857142857142857, 'ROUGE-2': 0.0, 'ROUGE-L': 0.2857142857142857, 'BLEU': 0.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the evaluation function\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "\n",
        "def evaluate_generated_answer(reference, generated):\n",
        "    \"\"\"\n",
        "    Evaluate the generated answer using ROUGE and BLEU scores.\n",
        "    Handles cases where the input is nested (e.g., dictionaries instead of plain strings).\n",
        "    \"\"\"\n",
        "    # Extract the plain text if inputs are nested dictionaries\n",
        "    if isinstance(reference, dict):\n",
        "        reference = reference.get(\"content\", reference)\n",
        "    if isinstance(generated, dict):\n",
        "        generated = generated.get(\"content\", generated)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, generated)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sacrebleu.corpus_bleu([generated], [[reference]])\n",
        "\n",
        "    return {\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"].fmeasure,\n",
        "        \"ROUGE-2\": rouge_scores[\"rouge2\"].fmeasure,\n",
        "        \"ROUGE-L\": rouge_scores[\"rougeL\"].fmeasure,\n",
        "        \"BLEU\": bleu_score.score,\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "reference_answer = {\"content\": \"Therapeutics is the treatment of disease\"}\n",
        "generated_answer = response[\"answer\"]\n",
        "\n",
        "# Evaluate the answers\n",
        "evaluation_scores = evaluate_generated_answer(reference_answer, generated_answer)\n",
        "print(\"Evaluation Scores:\", evaluation_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.42.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.2.3)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (5.29.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (19.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.4.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair<6,>=4.0->streamlit) (1.25.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\a13na\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\A13na\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
